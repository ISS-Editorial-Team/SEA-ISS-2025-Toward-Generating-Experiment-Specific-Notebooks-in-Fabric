{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Results\n",
    "To evaluate our RAG model approach, we ingested a (curated) set of FABRIC Notebooks into our Vector Database.  We then tested against several of the LLMs mentioned earlier by giving them three distinct queries to answer (i.e., questions to generate FABRIC answers (python scripts) for).   We then manually  ranked the correctness of the LLM output using a simple scoring system ranging from “Useless” to “Code is correct”.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries\n",
    "We chose three queries for testing, representing three levels of complexities. The queries mimic  commonly asked questions by FABRIC users of various levels of expertise.\n",
    "\n",
    "- Easy: How can I check what slices I have?\n",
    "- Intermediate: How can I look up when my slices expire and extend them by 20 days?\n",
    "- Advanced: How can I create a slice with two nodes connected with L3 networks using Basic NICs and do cpu pinning and NUMA tuning and launch iperf3 tests between them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring system\n",
    "We ran each query once for each model, using the temperature of 0.0 as noted above. FABRIC software team members assigned a score of 0~4 described as follows. In many cases, we ran the generated code to confirm that it successfully reserved the correct set of resources:\n",
    "\n",
    "0: Useless. Largely a result of hallucination. \n",
    "\n",
    "1: Contain some correct elements but largely incorrect\n",
    "\n",
    "2: About half correct (some useful sections/elements, but still far)\n",
    "\n",
    "3: Mostly correct – code would be a good starting point and usable with minor corrections\n",
    "\n",
    "4: Code is correct and runs without any edits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Results\n",
    "\n",
    "The effectiveness of using RAG is clearly demonstrated by the results. Without RAG, no model was able to generate  error-free code even for the easy query (that could be written in as short as 3 lines of code). For the intermediate-level question, there was little  resemblance between the no–RAG generated code and the correct result; and for Question 3 (advanced level), all outputs could be described as hallucinations. Without RAG, therefore, even the highly-rated LLMs are incapable of coherent and useful code for FABRIC users. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 5: Comparison of RAG vs. no RAG outputs for the same question using the same LLM**\n",
    "\n",
    "Output on the right (with RAG) is correct. Notice the wrong import statment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![notebook_example](images/easy_Q_output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
